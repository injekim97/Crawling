# Crawling


**instagram**  
-> 인턴 다니는 회사에서 인플루언서의 팔로워 팔로잉 수를 매일 저장하며, 게시글에 어떤 사람이 유입되는지를 알기 위한 프로젝트  
-> 21.05.12에 위의 조건1-5를 포함한 최종 구현 파일 3개 업로드   

**instagram Logic**  
-> [Facebook] 파일명(CSV) -> facebook으로 대체 로그인 후 인플루언서 정보를 CSV로 저장 -> [instgram] 파일명(CSV) -> instgram으로 로그인 후 인플루언서 정보를 CSV로 저장 -> 0512_1800 [최종] - CSV 파일을 읽어와서 비교 -> 위 두가지 중 하나를 실행하여 CSV로 저장된 파일을 읽어와서 현재의 인플루언서와 비교      
***   


**의회 사이트**  
-> 부산,대구,대전,광주,인천,울산의 국회의원 프로필 데이터들을 CSV로 저장함  
***   



**law-talk**
->  로톡 "사기" 키워드에 관한 데이터 크롤링   
-> lawtalk_craw_1day_ago.ipynb : PC에서 로톡의 사기 키워드에 관한 24시간(1일 이전) 데이터만 DB에 저장   
-> lawtalk-craw-after1days.ipynb : 리눅스에서 로톡의 사기 키워드에 관한 24시간 이후(1일 이후) 데이터만 DB에 저장   
***   


**법률구조공단(KLAC)**  
-> 법률구조공단 크롤링(PC).ipynb : PC에서 돌아가는 크롤링 파일   
-> KLAC.craw.py : 리눅스 서버에서 돌아가는 크롤링 파일   
***   


**서울중앙지법무사(KABL)**    
-> 서울중앙지법무사협회 : 서울중앙지법무사협회 데이터를 PC에서 DB로 수집하는 코드   
-> KABL.py : 서울중앙지법무사협회 데이터를 리눅스에서 수집하는 코드   
***   


**2021.11.25**   
crontab 설정 방법 및 sh파일.txt  
-> 24시간 동안 리눅스에서 크롤링을 돌리기 위한 Job 스케줄러  
***   


**2022.01.11**   
-> 면접 준비를 위한 해당 기업의 면접 데이터 크롤링을 위해 jobplanet 리포지토리 생성   